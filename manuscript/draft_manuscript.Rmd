---
title: "How many researchers use 'boilerplate' statistical analysis sections?"
author: "Nicole White, Richi Nayak, Adrian Barnett"
date: "21/05/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
link-citations: yes
bibliography: references.bib
subtitle: An observational study of papers published in _PLOS ONE_.
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(pander)
library(ggplot2)
panderOptions('table.emphasize.rownames', FALSE)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('table.split.table', Inf)
panderOptions('table.split.cells', Inf)
panderOptions('big.mark', ',')
panderOptions('table.emphasize.rownames', FALSE)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('table.split.table', Inf)
panderOptions('table.split.cells', Inf)
panderOptions('big.mark', ',')
panderOptions('keep.line.breaks', TRUE)
panderOptions('table.alignment.default','left')
```

An ideal statistical analysis will use appropriate methods to create insights from the data and inform the research questions. Unfortunately many current statistical analyses are far from ideal, with many researchers using the wrong methods, misinterpreting the results, or failing to adequately check their assumptions [@Goodman2008;@Leek2017]. Some researchers take a "mechanistic" approach to statistics, copying the few methods they know regardless of their appropriateness, and then going through the motions of the analysis [@Stark2018]. 

Many researchers may not have received adequate training in research methods, and statistics is something they do with trepidation and even ignorance [@Altman1994;@King2019]. 
However, using the wrong statistical methods can cause real harm [@Altman1994] and bad statistical practices are being to used abet weak science [@Stark2018].
Statistical mistakes are a key source of waste in research and partly explain the current reproducibility crisis in science [@Allison2016]. Even when the correct methods are used, many researchers fail to describe them adequately, making it difficult to reproduce the results [@Ernst2017;@Zhou2018].

The International Committee of Medical Journal Editors recommend that researchers should: “Describe statistical methods with enough detail to enable a knowledgeable reader with access to the original data to judge its appropriateness for the study and to verify the reported results” [@ICMJE2019]. Although the general lack of statistical understanding from both authors and reviewers means this recommendation may not be checked.
A recent survey of editors found that only 23% of health and medical journals used expert statistical review for all articles [@Hardwicke2020], which was little different from a survey from 22 years ago [@Goodman1998].

As statisticians we have heard researchers admit that they sometimes copy-and-paste their statistical methods sections from other papers, regardless of whether they are appropriate. 
The aim of this paper is to use text-mining methods to estimate the extent that researchers are using 'boilerplate' statistical methods sections.
Use of these methods sections indicates that little thought has gone into the statistical analysis.


# Methods

## Data sources

### Public Library of Science (PLOS ONE)

PLOS ONE is a large open access journal that publishes original research across the fundamental sciences. Article submissions are handled by an academic editor who selects peer reviewers based on their self-nominated area(s) of expertise. Submissions do not undergo formal statistical review. Instead, reviewers are required to assess submissions against several publication criteria, including whether “Experiments, statistics, and other analyses are performed to a high technical standard and are described in sufficient detail”. Authors are encouraged to follow published reporting guidelines to ensure that chosen statistical methods are appropriate for the study design, and adequate details are provided to enable independent replication of results. For example, the EQUATOR network has developed reporting guidelines for study designs commonly used in health research, in part because of the long history of poor statistical application and reporting in health and medical journals [@Altman2016].

All articles published in PLOS ONE are freely accessible via the PLOS Application Programming Interface (API). This functionality enabled us to conduct semi-automated searches of full-text articles and analyse data on individual records, including text content and general attributes such as publication date and field(s) of research. Our search strategy comprised of targeted API searches followed by article filtering based on available section headings. 

_Step 1_: Targeted API searches. API searches were completed using the R package ‘rplos’ [@rplos]. Search queries targeted the presence of analysis-related terms anywhere within a full-text article. Individual search terms combined the words “data” or “statistical”, with one of: “analysis”, “analyses”, “method”, “methodology” or “model(l)ing”. Search terms were intended to be broad whilst keeping search results to a manageable number for full-text review (see Step 2). By allowing terms to appear anywhere within the article, we accounted for the possibility of relevant text being placed in different sections, for example, in the Material and Methods section versus Supplementary Information. Search results were indexed by a unique Digital Object Identifier (DOI). Attribute data collected per DOI included journal volume, subject classification(s) and total article views since publication.

_Step 2_: Partial matching on section headings. Full text XML data for all search results were downloaded and combined into a single dataset, organised by DOI and subsection heading(s). Since PLOS ONE does not prescribe standardised headings to preface statistical methods sections, we performed partial matching on available headings against frequently used terms in initial search results: ‘Statistical analysis’, ‘Statistical analyses’, ‘Statistical method’, ‘Statistics’, ‘Data analysis’ and ‘Data analyses’. To determine the reliability of our chosen filters, we manually reviewed full text data extracted for a random sample of XXX articles that were not matched (File S1).[TODO…finish this thought…]

### Australia and New Zealand Clinical Trials Registry (ANZCTR)

ANZCTR was established in 2005 as part of a coordinated global effort to improve research quality and transparency in clinical trials reporting. Researchers responsible for overseeing a clinical trial are expected to register full details before commencing participant enrolment. Details required for registration follow a standardised template (reference or supp file), which covers important details about participant eligibility, intervention(s) being evaluated, study design and outcomes. As part of study design, researchers are required to provide a summary of statistical methods and planned analyses. Upon successful registration, the trial is assigned a unique registration number for use in future reporting in scientific publications. All trials registered on AZNCTR are publicly available and can be searched for via an online portal.
[TODO: details of how stat.section text was extracted for analysis]


## Full-text processing
Text cleaning aimed to standardise notation and statistical terminology, whilst minimising changes to article style and formatting. Full details of text cleaning steps undertaken and corresponding R code are provided in Supplementary File X.

Mathematical notation including Greek letters was converted from Unicode characters to plain text. For example, the Unicode characters corresponding to $\theta$ (<U+03B8>) was replaced with ‘theta’. Similarly, common symbols outside of Unicode blocks including ‘%’ (percent) and ‘<’ (‘less-than’) were converted into plain text, using functions available in the ‘textclean’ package [@textclean]. General formatting including carriage returns, punctuation marks, in-text references (e.g. [42]) centred equations and other non-ascii characters were removed. Text contained inside brackets was retained in the dataset to maximise content for analysis, with brackets removed.

We compiled an extensive list of statistical terms to standardise descriptions of statistical methods reported across included record in both datasets. An initial list was compiled by calculating individual word frequencies and identifying relevant terms that appeared at least 100 times. Further terms were sourced from index searches of statistics reference textbooks [ref]. The final list is provided as Supplementary Material. Possible variants including plurals (e.g. ‘chi-squares’) unhyphenated (e.g chi square) and combined (e.g. chisquare) terms were transformed to singular, hyphenated form (e.g chi-square). Common statistical tests were also hyphenated (e.g. hosmer lemeshow to hosem-lemeshow).

As a final step, common stop words including pronouns, contractions and selected prepositions were removed. We chose to keep certain stop words that, if excluded, may have changed the context of statistical method(s) being described (e.g. ‘between’, ‘against’).


<!-- Questions: -->

<!-- * What questions are asked of PLOS ONE Reviewers? -->

<!-- - Is the manuscript technically sound, and do the data support the conclusions? -->
<!-- - Has the statistical analysis been performed appropriately and rigorously? -->
<!-- - Does the manuscript adhere to the PLOS Data Policy? -->
<!-- - Is the manuscript presented in an intelligible fashioin and written in standard English? -->


<!-- * Reference updates to stats reporting guidelines; e.g see: -->

<!-- - https://everyone.plos.org/2019/09/26/new-plos-one-statistical-reporting-guidelines/. -->
<!-- - https://web.archive.org/web/20190607174803/https://journals.plos.org/plosone/s/submission-guidelines -->
<!-- - https://web.archive.org/web/20150507175314/https://journals.plos.org/plosone/s/submission-guidelines -->


## Clustering algorithm


<!-- # Results -->

<!-- ```{r source, message=F,echo=F} -->
<!-- load('data/search_results_n.RData') -->
<!-- ``` -->



<!-- * total records found: 178654; 131847 unique DOIs -->
<!-- * total stats sections after filtering: 111731 (85%) -->
<!-- * Plot over time. Currently double counting if more than one search query for same doi. -->

<!-- ```{r echo=F,fig.cap="\\label{fig:search-results-n}Figure caption here"} -->
<!-- plot_dat_n = n_by_searchterm_volume %>% group_by(volume) %>%  -->
<!--   summarise(n_matches = sum(n_unique)) %>% right_join(.,n_by_volume %>% select(-n_records),by='volume') %>%  -->
<!--   gather(variable,value,-volume) -->

<!-- plot_dat_n %>% ggplot(.,aes(x=volume,y=value,group=variable))+ -->
<!--   geom_bar(stat='identity',position='dodge',aes(fill=variable)) -->

<!-- ``` -->



# Discussion

The first line in many statistical analysis sections was the software used, implying that the software is the most important detail. As Doug Altman said, "Many people think that all you need to do statistics is a computer and appropriate software" [@Altman1994]. This is not the case, and whilst it is important for researchers to mention the software and version used for reproducibility purposes, it is a relatively minor detail compared with detailing what methods were used and why.

Despite the extensive array of tests available, many authors are reporting the same few methods.

## Limitations

We did not check whether papers used the correct methods, and for some simple studies a 'boilerplate' statistical methods section would be fine.

We examined papers where there was a statistics section, and we missed papers that used statistical analysis but did not include a statistical analysis section. Reiterate outcomes of random sample checking here.

We only examined one journal and hence our results may not be generalisable to all journals, especially those that use a statistical reviewer for all papers.

# References

