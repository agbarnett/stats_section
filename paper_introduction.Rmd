---
title: "How many researchers use 'boilerplate' statistical analysis sections?"
author: "Nicole White, Richi Nayak, Adrian Barnett"
date: "21/05/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
link-citations: yes
bibliography: references.bib
subtitle: An observational study of papers published in _PLOS ONE_.
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(pander)
library(ggplot2)
library(DiagrammeR) 
panderOptions('table.emphasize.rownames', FALSE)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('table.split.table', Inf)
panderOptions('table.split.cells', Inf)
panderOptions('big.mark', ',')
panderOptions('table.emphasize.rownames', FALSE)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('table.split.table', Inf)
panderOptions('table.split.cells', Inf)
panderOptions('big.mark', ',')
panderOptions('keep.line.breaks', TRUE)
panderOptions('table.alignment.default','left')
```

An ideal statistical analysis will use appropriate methods to create insights from the data and inform the research questions. Unfortunately many current statistical analyses are far from ideal, with many researchers using the wrong methods, misinterpreting the results, or failing to adequately check their assumptions [@Goodman2008;@Leek2017]. Some researchers take a "mechanistic" approach to statistics, copying the few methods they know regardless of their appropriateness, and then going through the motions of the analysis [@Stark2018]. 

Many researchers may not have received adequate training in research methods, and statistics is something they do with trepidation and even ignorance [@Altman1994;@King2019]. 
However, using the wrong statistical methods can cause real harm [@Altman1994] and bad statistical practices are being to used abet weak science [@Stark2018].
Statistical mistakes are a key source of waste in research and partly explain the current reproducibility crisis in science [@Allison2016]. Even when the correct methods are used, many researchers fail to describe them adequately, making it difficult to reproduce the results [@Ernst2017;@Zhou2018].

The International Committee of Medical Journal Editors recommend that researchers should: “Describe statistical methods with enough detail to enable a knowledgeable reader with access to the original data to judge its appropriateness for the study and to verify the reported results” [@ICMJE2019]. Although the general lack of statistical understanding from both authors and reviewers means this recommendation may not be checked.
A recent survey of editors found that only 23% of health and medical journals used expert statistical review for all articles [@Hardwicke2020], which was little different from a survey from 22 years ago [@Goodman1998].

As statisticians we have heard researchers admit that they sometimes copy-and-paste their statistical methods sections from other papers, regardless of whether they are appropriate. 
The aim of this paper is to use text-mining methods to estimate the extent that researchers are using 'boilerplate' statistical methods sections.
Use of these methods sections indicates that little thought has gone into the statistical analysis.


# Methods

## Data source

We chose _PLOS ONE_ because it is a large multi-disciplinary open access journal, and because the journal laudibly make the full text of papers easily available for analysis via an API. This enabled us to isolate the statistical analysis section of the paper, and also collect descriptive data on the paper (e.g., field of research).

<!--- https://journals.plos.org/plosone/s/editorial-and-peer-review-process --->
<!--- https://journals.plos.org/plosone/s/criteria-for-publication --->
The criteria for publication include, "Experiments, statistics, and other analyses are performed to a high technical standard and are described in sufficient detail".
_PLOS ONE_ does not use a separate statistical peer reviewer. Articles are handled by an academic editor who selects peer reviewers. 
_PLOS ONE_ expect authors to use reporting guidelines from the EQUATOR network, which were designed in part because of the long history of poor statistical application and reporting in health and medical journals [@Altman2016].

We used the 'rplos' package to read the data from the API into R [@rplos].

Questions:

* What questions are asked of PLOS ONE Reviewers?

- Is the manuscript technically sound, and do the data support the conclusions?
- Has the statistical analysis been performed appropriately and rigorously?
- Does the manuscript adhere to the PLOS Data Policy?
- Is the manuscript presented in an intelligible fashioin and written in standard English?


## Search strategy
Figure \ref{fig:search-strategy} outlines the main steps of our search strategy. Searches were completed using the 'rplos' package [@rplos]. All full-text research articles published in _PLOS ONE_ since 2006 were eligible for inclusion (last accessed: 5 June 2020). 

Records were first identified using combinations of analysis-related search terms that could appear anywhere in the full-text. Metadata for each record consisted of the digital objected identifier (DOI), article title, journal volume, subject area(s) and total article views since online publication. For each DOI returned, Full text XML was extracted and organsied by listed section and subsection headings. In order to consistently identify descriptions of statistical analyses undertaken, subsection headings were filtered using common titles (e.g. 'Data analysis'; 'Statisical methods'). Records that returned more than one heading at the filtering stage were combined (?). Filtering resulted were cleaned using the 'textclean' package [@textclean] (see Supplementary File 1 for full details)


```{r echo=F,fig.cap='\\label{fig:search-strategy}Search strategy',fig.align='center',fig.height=5}
knitr::include_graphics("figures/search_strategy.png")

```

# Results
```{r source, message=F,echo=F}
load('data/search_results_n.RData')
```

* total records found: 178654; 131847 unique DOIs
* total stats sections after filtering: 111731 (85%)
* Plot over time. Currently double counting if more than one search query for same doi.

```{r echo=F,fig.cap="\\label{fig:search-results-n}Figure caption here"}
plot_dat_n = n_by_searchterm_volume %>% group_by(volume) %>% 
  summarise(n_matches = sum(n_unique)) %>% right_join(.,n_by_volume %>% select(-n_records),by='volume') %>% 
  gather(variable,value,-volume)

plot_dat_n %>% ggplot(.,aes(x=volume,y=value,group=variable))+
  geom_bar(stat='identity',position='dodge',aes(fill=variable))

```

# Discussion

The first line in many statistical analysis sections was the software used, implying that the software is the most important detail. As Doug Altman said, "Many people think that all you need to do statistics is a computer and appropriate software" [@Altman1994]. This is not the case, and whilst it is important for researchers to mention the software and version used for reproducibility purposes, it is a relatively minor detail compared with detailing what methods were used and why.

Despite the extensive array of tests available, many authors are reporting the same few methods.

## Limitations

We did not check whether papers used the correct methods, and for some simple studies a 'boilerplate' statistical methods section would be fine.

We examined papers where there was a statistics section, and we missed papers that used statistical analysis but did not include a statistical analysis section. 

We only examined one journal and hence our results may not be generalisable to all journals, especially those that use a statistical reviewer for all papers.

# References

